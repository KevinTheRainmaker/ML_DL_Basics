{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UTFD_L9C5_Tokenizing_Text.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOhJeF+n3J07xG3MBe+sJ4t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KevinTheRainmaker/ML_DL_Basics/blob/master/Udacity%3A%20Intro%20to%20TensorFlow%20for%20DL/UTFD_L9C5_Tokenizing_Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Au6q9qc8NGsF"
      },
      "source": [
        "# Tokenizing text and Creating sequences for sentences\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_jpuZ7tNPFw"
      },
      "source": [
        "## Import the Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mYsL0CLM3ei"
      },
      "source": [
        "# Import the Tokenizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTJIMa-cNe59"
      },
      "source": [
        "## Sentences to Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XNanBxWNYum"
      },
      "source": [
        "sentences = [\n",
        "             'I saw someone I knew wherever I went during the fair',\n",
        "             'Guests can help themselves to refreshments whenever they wish',\n",
        "             'She will do her best to answer them as thoroughly as possible',\n",
        "             'She and I go to the fair as a guests',\n",
        "             'She will read and edit the articles for the next fair'\n",
        "]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWs5FplrPEvB"
      },
      "source": [
        "## Tokenize the words\n",
        "\n",
        "The first step to preparing text to e used in a machine learning model is to tokenize the text.\n",
        "\n",
        "In other words, generate numbers for the words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24OtL8B4Odic"
      },
      "source": [
        "# Optionally set the max number of words to tokenize.\n",
        "# The out of vocabulary (OOV) token represents words that are not in the index.\n",
        "# Call `fit_on_text()` on the tokenizer to generate unique numbers for each word.\n",
        "\n",
        "tokenizer = Tokenizer(num_words= 100, oov_token = '<OOV>')\n",
        "tokenizer.fit_on_texts(sentences)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMjm7CgHQAEy"
      },
      "source": [
        "## View the word index\n",
        "\n",
        "After tokenizing the text, the tokenizer has a word index that contains key-value pairs for all the words and their numbers.\n",
        "\n",
        "The word is the key, and the number is the value.\n",
        "\n",
        "Notice that the OOV token is the first entry."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GR9e7a2P8eV",
        "outputId": "bea2c217-fee5-4f88-fa16-2510980110ac"
      },
      "source": [
        "# Examine the word index\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<OOV>': 1, 'i': 2, 'the': 3, 'fair': 4, 'to': 5, 'she': 6, 'as': 7, 'guests': 8, 'will': 9, 'and': 10, 'saw': 11, 'someone': 12, 'knew': 13, 'wherever': 14, 'went': 15, 'during': 16, 'can': 17, 'help': 18, 'themselves': 19, 'refreshments': 20, 'whenever': 21, 'they': 22, 'wish': 23, 'do': 24, 'her': 25, 'best': 26, 'answer': 27, 'them': 28, 'thoroughly': 29, 'possible': 30, 'go': 31, 'a': 32, 'read': 33, 'edit': 34, 'articles': 35, 'for': 36, 'next': 37}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywN8pD9-QfF-",
        "outputId": "9a2f1f2d-7bb4-4946-c018-e3fd61e2fd88"
      },
      "source": [
        "# Get the number for a given word\n",
        "print(word_index['answer'])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0KKJuqqRDMo"
      },
      "source": [
        "## Create sequences for the sentences\n",
        "\n",
        "After tokenizing the words, the word index contains a unique number for each word.\n",
        "\n",
        "However, the numbers in the word index are not ordered, unlikely the words in a sentences.\n",
        "\n",
        "So after tokenizing the words, the next step is to generate sequences for the sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDJgo6SeQ_KE",
        "outputId": "94fb80d8-278c-46c0-befe-64eb093ed752"
      },
      "source": [
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "print(sequences)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2, 11, 12, 2, 13, 14, 2, 15, 16, 3, 4], [8, 17, 18, 19, 5, 20, 21, 22, 23], [6, 9, 24, 25, 26, 5, 27, 28, 7, 29, 7, 30], [6, 10, 2, 31, 5, 3, 4, 7, 32, 8], [6, 9, 33, 10, 34, 3, 35, 36, 3, 37, 4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBp3FXpdSQoU"
      },
      "source": [
        "## Sequence sentences that contain words that are not in the word index\n",
        "\n",
        "How about the sentences that contains word that not in the index?\n",
        "\n",
        "Hint: The OOV token is the first(1) entry in the word index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tw9C-cr4SG3M",
        "outputId": "86f46168-8e00-4316-9228-25a6eb2e7e92"
      },
      "source": [
        "sentences2 = [\n",
        "              'He will be honored for his research at the fair',\n",
        "              'The power cable provided with the computer is compatible with other device'\n",
        "              ]\n",
        "sequences2 = tokenizer.texts_to_sequences(sentences2)\n",
        "print(sequences2)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 9, 1, 1, 36, 1, 1, 1, 3, 4], [3, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oD4y1hYTz4n"
      },
      "source": [
        "The OOV token represented as '1' in sequences."
      ]
    }
  ]
}