{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UTFD_L9C7_Preparing_Text_to_Use_with_TensorFlow_Models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOwiJR8kyu2KE1k8CGaCW2k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KevinTheRainmaker/ML_DL_Basics/blob/master/Udacity%3A%20Intro%20to%20TensorFlow%20for%20DL/UTFD_L9C7_Preparing_Text_to_Use_with_TensorFlow_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-cjT0K7xGJL"
      },
      "source": [
        "#Preparing text to use with TensorFlow models\n",
        "\n",
        "The high level steps to prepare text to be used in a machine learning model are:\n",
        "\n",
        "1. Tokenizing the words to get numerical values\n",
        "2. Create numerical sequences of the sentences\n",
        "3. Adjust the sequences to all be the same length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LECPRjZzNfm"
      },
      "source": [
        "## Import the classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRVZ0rBFcfWO"
      },
      "source": [
        "# Import Tokenizer and pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbcAA7XGzivv"
      },
      "source": [
        "## Sentences to Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKYQIHI-ziLT"
      },
      "source": [
        "sentences = [\n",
        "             'I saw someone I knew wherever I went during the fair',\n",
        "             'Guests can help themselves to refreshments whenever they wish',\n",
        "             'She will do her best to answer them as thoroughly as possible',\n",
        "             'She and I go to the fair as a guests',\n",
        "             'She will read and edit the articles for the next fair'\n",
        "]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KH92SoQHzsi8",
        "outputId": "a0995718-130e-48a8-b1ed-a81855ef3cdd"
      },
      "source": [
        "print(sentences)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I saw someone I knew wherever I went during the fair', 'Guests can help themselves to refreshments whenever they wish', 'She will do her best to answer them as thoroughly as possible', 'She and I go to the fair as a guests', 'She will read and edit the articles for the next fair']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLLPbL5AzvhR"
      },
      "source": [
        "## Create the Tokenizer and define an OOV token\n",
        "\n",
        "When creating the Tokenizer, we can specify the max number of words in the dictionary and a token to represent words that are OOV.\n",
        "\n",
        "This OOV token will be used when we create sequences for sentences that contain words that are not in the word index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZ0wVTjPzumc"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=100, oov_token='<OOV>')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLWMSH1q1Ojg"
      },
      "source": [
        "## Tokenize the words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Us3H8nV0biu",
        "outputId": "c3e6cefc-419b-4bff-c318-5b271f188e14"
      },
      "source": [
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<OOV>': 1, 'i': 2, 'the': 3, 'fair': 4, 'to': 5, 'she': 6, 'as': 7, 'guests': 8, 'will': 9, 'and': 10, 'saw': 11, 'someone': 12, 'knew': 13, 'wherever': 14, 'went': 15, 'during': 16, 'can': 17, 'help': 18, 'themselves': 19, 'refreshments': 20, 'whenever': 21, 'they': 22, 'wish': 23, 'do': 24, 'her': 25, 'best': 26, 'answer': 27, 'them': 28, 'thoroughly': 29, 'possible': 30, 'go': 31, 'a': 32, 'read': 33, 'edit': 34, 'articles': 35, 'for': 36, 'next': 37}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFiWRIHP1ZmO"
      },
      "source": [
        "## Create sequences for the sentences\n",
        "\n",
        "After tokenizing the words, the word index contains a unique number for each word.\n",
        "\n",
        "However, the numbers in the word index are not ordered, unlikely the words in a sentences.\n",
        "\n",
        "So after tokenizing the words, the next step is to generate sequences for the sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOSVl6Uw1X9S",
        "outputId": "3a24e9d4-0798-4e05-a4de-85c12711d04a"
      },
      "source": [
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "print(sequences)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2, 11, 12, 2, 13, 14, 2, 15, 16, 3, 4], [8, 17, 18, 19, 5, 20, 21, 22, 23], [6, 9, 24, 25, 26, 5, 27, 28, 7, 29, 7, 30], [6, 10, 2, 31, 5, 3, 4, 7, 32, 8], [6, 9, 33, 10, 34, 3, 35, 36, 3, 37, 4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OC68flNa3RZS"
      },
      "source": [
        "## Make the sequences all the same length\n",
        "\n",
        "Later, when you feed the sequences into a neural network to train a model, the sequences all nedd to be uniform in size. \n",
        "\n",
        "Currently the sequences have varied lengths, so the next step is to make them all be the same size, either by paddin or truncating.\n",
        "\n",
        "</br>\n",
        "\n",
        "### Padding & Truncating\n",
        "Using `pad_sequences()` to padding or truncating the sequences to make them all be the same length. By default, the process goes at the start of the sequences, but you can specify the start direction.\n",
        "\n",
        "If you don't provide the max length, then the sequence are padded to match the length of longest sentence(No truncating).\n",
        "\n",
        "</br>\n",
        "\n",
        "[[All about the options about padding and truncating]](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BioyhGxD3QM_",
        "outputId": "9cdc6793-fb02-4f75-a21c-499ecd9b31c6"
      },
      "source": [
        "padded = pad_sequences(sequences)\n",
        "print(\"Word Index: \", word_index)\n",
        "print(\"Sequences: \", sequences)\n",
        "print(\"Padded Sequences: \", padded)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Index:  {'<OOV>': 1, 'i': 2, 'the': 3, 'fair': 4, 'to': 5, 'she': 6, 'as': 7, 'guests': 8, 'will': 9, 'and': 10, 'saw': 11, 'someone': 12, 'knew': 13, 'wherever': 14, 'went': 15, 'during': 16, 'can': 17, 'help': 18, 'themselves': 19, 'refreshments': 20, 'whenever': 21, 'they': 22, 'wish': 23, 'do': 24, 'her': 25, 'best': 26, 'answer': 27, 'them': 28, 'thoroughly': 29, 'possible': 30, 'go': 31, 'a': 32, 'read': 33, 'edit': 34, 'articles': 35, 'for': 36, 'next': 37}\n",
            "Sequences:  [[2, 11, 12, 2, 13, 14, 2, 15, 16, 3, 4], [8, 17, 18, 19, 5, 20, 21, 22, 23], [6, 9, 24, 25, 26, 5, 27, 28, 7, 29, 7, 30], [6, 10, 2, 31, 5, 3, 4, 7, 32, 8], [6, 9, 33, 10, 34, 3, 35, 36, 3, 37, 4]]\n",
            "Padded Sequences:  [[ 0  2 11 12  2 13 14  2 15 16  3  4]\n",
            " [ 0  0  0  8 17 18 19  5 20 21 22 23]\n",
            " [ 6  9 24 25 26  5 27 28  7 29  7 30]\n",
            " [ 0  0  6 10  2 31  5  3  4  7 32  8]\n",
            " [ 0  6  9 33 10 34  3 35 36  3 37  4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YQHjyV-SQ4j"
      },
      "source": [
        "\\* Sentence 3 is the longest and other sentences are padded to match sentence 3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWZXzQl3SJgE",
        "outputId": "58da5165-fd5d-43ed-f312-eb8171f8a940"
      },
      "source": [
        "# Specify a max length for the padded sequences\n",
        "padded = pad_sequences(sequences, maxlen=15)\n",
        "print(padded)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  0  0  0  2 11 12  2 13 14  2 15 16  3  4]\n",
            " [ 0  0  0  0  0  0  8 17 18 19  5 20 21 22 23]\n",
            " [ 0  0  0  6  9 24 25 26  5 27 28  7 29  7 30]\n",
            " [ 0  0  0  0  0  6 10  2 31  5  3  4  7 32  8]\n",
            " [ 0  0  0  0  6  9 33 10 34  3 35 36  3 37  4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlrtQgaASlVl"
      },
      "source": [
        "Also the sentence 3 is padded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4QGXfIuSkdT",
        "outputId": "2d45f67b-450b-4431-bfa1-61777ff0b1bb"
      },
      "source": [
        "# Put the padding at the end of the sequences\n",
        "padded = pad_sequences(sequences, maxlen=15, padding='post')\n",
        "print(padded)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 2 11 12  2 13 14  2 15 16  3  4  0  0  0  0]\n",
            " [ 8 17 18 19  5 20 21 22 23  0  0  0  0  0  0]\n",
            " [ 6  9 24 25 26  5 27 28  7 29  7 30  0  0  0]\n",
            " [ 6 10  2 31  5  3  4  7 32  8  0  0  0  0  0]\n",
            " [ 6  9 33 10 34  3 35 36  3 37  4  0  0  0  0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9I1YBYCSyQi"
      },
      "source": [
        "Padding(Additional zeros) is added to the end of sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SSR4tgWSxhc",
        "outputId": "d4cd5065-7bc1-4590-9776-5b58340452ab"
      },
      "source": [
        "# Limit the length of the sequences, you will see some sequences get truncated\n",
        "padded = pad_sequences(sequences, maxlen = 6)\n",
        "print(padded)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[14  2 15 16  3  4]\n",
            " [19  5 20 21 22 23]\n",
            " [27 28  7 29  7 30]\n",
            " [ 5  3  4  7 32  8]\n",
            " [ 3 35 36  3 37  4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwggVdIRTNkS"
      },
      "source": [
        "## OOV Examples\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6xEZd3iTLRX",
        "outputId": "e80f9207-da02-44d5-d9b4-b24256e607f1"
      },
      "source": [
        "# Try turning sentences that contain words that \n",
        "# aren't in the word index into sequences.\n",
        "# Add your own sentences to the test_data\n",
        "test_data = [\n",
        "    \"my best friend's favorite ice cream flavor is strawberry\",\n",
        "    \"my dog's best friend is a manatee\"\n",
        "]\n",
        "print (test_data)\n",
        "\n",
        "# Remind ourselves which number corresponds to the\n",
        "# out of vocabulary token in the word index\n",
        "print(\"<OOV> has the number\", word_index['<OOV>'], \"in the word index.\")\n",
        "\n",
        "# Convert the test sentences to sequences\n",
        "test_seq = tokenizer.texts_to_sequences(test_data)\n",
        "print(\"\\nTest Sequence = \", test_seq)\n",
        "\n",
        "# Pad the new sequences\n",
        "padded = pad_sequences(test_seq, maxlen=10)\n",
        "print(\"\\nPadded Test Sequence: \")\n",
        "\n",
        "# Notice that \"1\" appears in the sequence wherever there's a word \n",
        "# that's not in the word index\n",
        "print(padded)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"my best friend's favorite ice cream flavor is strawberry\", \"my dog's best friend is a manatee\"]\n",
            "<OOV> has the number 1 in the word index.\n",
            "\n",
            "Test Sequence =  [[1, 26, 1, 1, 1, 1, 1, 1, 1], [1, 1, 26, 1, 1, 32, 1]]\n",
            "\n",
            "Padded Test Sequence: \n",
            "[[ 0  1 26  1  1  1  1  1  1  1]\n",
            " [ 0  0  0  1  1 26  1  1 32  1]]\n"
          ]
        }
      ]
    }
  ]
}